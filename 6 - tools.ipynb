{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "placeholder-image",
   "metadata": {},
   "source": [
    "![](../assets/placeholder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d752e-589e-4d79-a39c-32c11bdfbcbf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is designed to guide you through the process of building tools for agents using Langchain. Tools are a key part of agents and a great way to add more capabilities to an agent and the agent can combine these in useful ways. In this notebook, we will explore how to create custom tools and integrate them with agents to enhance their capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc0585b-34e6-417a-af31-2b0c1c4be2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2433c55-9555-4ac5-9d1f-261a1321d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=10000)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c613b51-57cd-403f-b001-6eae037ef5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ec13f64-29d5-4185-9418-76039b39856c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query',\n",
       "  'description': 'query to look up on wikipedia',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8f15b4-88cb-4251-a408-a431a40c0029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Intelligent agent\\nSummary: In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner. It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.\\n\\nLeading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".\\nAn agent has an \"objective function\" that encapsulates all the IA\\'s goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function. For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA\\'s desired behavior, and an evolutionary algorithm\\'s behavior is shaped by a \"fitness function\". \\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as an abstract functional system similar to a computer program. Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users).\\n\\nPage: Devin AI\\nSummary: Devin AI is an autonomous artificial intelligence assistant tool created by Cognition Labs. Branded as an \"AI software developer\", the demo tool is notable for its software development abilities, including plan implementation, source code generation, and benchmark unit testing. The tool has received praise, concern, and skepticism over implications surrounding the future of artificial intelligence and software development.\\n\\n\\n\\nPage: AI alignment\\nSummary: In the field of artificial intelligence (AI), AI alignment research aims to steer AI systems toward a person\\'s or group\\'s intended goals, preferences, and ethical principles. An AI system is considered aligned if it advances its intended objectives. A misaligned AI system may pursue some objectives, but not the intended ones.\\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But that approach can create loopholes, overlook necessary constraints, or reward the AI system for merely appearing aligned.\\nMisaligned AI systems can malfunction and cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking). They may also develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their final given goals. Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is deployed and encounters new situations and data distributions.\\nToday, these problems affect existing commercial systems such as language models, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected, since these problems partially result from the systems being highly capable.\\nMany of the most-cited AI scientists, including Geoffrey Hinton, Yoshua Bengio, and Stuart Russell, argue that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI) and could endanger human civilization if misaligned.\\nAI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and social sciences.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run(\"AI agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cce4c-ed56-4fe1-b764-ee4605d906b6",
   "metadata": {},
   "source": [
    "#### Custom tools\n",
    "We don't just have to use the default tools from LangChain. We can define our own custom tools. A tool can be any method that has clearly defined inputs and outputs so that the agent can invoke it. Function that adds two numbers together, it can be a tool. Function that calls a custom company API, it can be a tool. Function that generates code from a custom LLM, it can be a tool. This really opens up a lot of opportunities to extend an agent with many capabilities. In this section, we will explore how to define and integrate custom tools with agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f544777-e503-4f20-99ed-452ee862dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# tool decorator is the easiest way to define a new tool\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285cbc5e-a166-46a8-8be7-e237bd5d14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a981b01-95b3-4eb4-8699-fbfd71d53c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4355"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run({'a':65, 'b':67})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29aae18-98e1-44eb-b494-efffd3b9f568",
   "metadata": {},
   "source": [
    "### Web retriever tool\n",
    "Let's build a tool that can fetch the content of a URL, this is something we've had to do in multiple previous notebooks. This tool will be particularly useful for agents that need to gather information from the web to answer questions or perform tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec554b08-197a-4f43-8e46-de376cde0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def fetch_url_content(url: str) -> str:\n",
    "    \"\"\"Use this tool to fetch content of a URL\"\"\"\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1900bc51-a950-4543-a1ac-966debef8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = fetch_url_content\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "content = tool.run({'url': url})\n",
    "content = str(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e1ee9-d2b5-4ef3-9812-0a520239cd6b",
   "metadata": {},
   "source": [
    "#### Summarizer tool\n",
    "Let's convert what we did in summarize notebook into a tool too. This tool will enable agents to generate concise summaries of text content, which can be useful for quickly understanding the gist of large documents or web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6a32ef8-d535-49f8-9a77-5247d182aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@tool\n",
    "def summarize_content(content: str) -> str:\n",
    "    \"\"\"Use this tool to generate the summary of a text based content\"\"\"\n",
    "    system_message = \"Please provide a concise summary for the content\"\n",
    "    human_message = \"Content: \" + str(content)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=human_message),\n",
    "    ]\n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    chain = model | parser\n",
    "    summary = chain.invoke(messages)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4a41a60-02d3-4d8e-b1e6-dd5a3ac38833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 814ddac2-c7bd-4fdd-9966-0c2c1beef439, but expected {'tool'} run.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The document titled \"LLM Powered Autonomous Agents\" by Lilian Weng, published on June 23, 2023, discusses the construction of autonomous agents using large language models (LLMs) as core controllers. It provides an in-depth overview of the system\\'s key components: planning, memory, and tool use. The planning component involves task decomposition and self-reflection. Memory is divided into short-term and long-term, facilitated by techniques like Maximum Inner Product Search (MIPS). Tool use extends LLM capabilities by integrating external APIs. The article also covers case studies such as ChemCrow and Generative Agents and mentions challenges in context length, planning, and reliability. Various proof-of-concept examples like AutoGPT and GPT-Engineer are highlighted, demonstrating the potential and limitations of LLM-powered autonomous agents.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool = summarize_content\n",
    "tool.run({\"content\": content})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb667e-a100-4be9-85b1-f4f833345cf9",
   "metadata": {},
   "source": [
    "We've managed to take the output from one tool and pass it to another. Which is actually super simple with just regular programming with functions. Wouldn't it be great if the AI can just figure out when this needs to happen and make decisions about different permutations across many tools to complete a task? That is what AI agents exactly do. They are able to run tools or generations in a loop on their own until the task is completed. That is what we're going to dive into next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
