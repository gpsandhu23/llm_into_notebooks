{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a5a431-028a-4657-b6e0-ede6af1e8a82",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is designed to guide you through the process of building a simple retrieval augmented generation application using Langchain, ChromaDB and OpenAI's GPT-4 LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf10802-7fdb-4e2c-b23d-e310b2fa900f",
   "metadata": {},
   "source": [
    "### RAG: Retrieval Augmented Generation\n",
    "One of the main applications of LLMs has been to build Q&A applications of custom content. LLMs are trained on a lot of data. However, sometimes we only want answers based on our own content. In order to make this work, we first have to ingest and store our content in a format that is easy to search and retrieve\n",
    "\n",
    "#### URL ---> Content ---> Split ---> Embed ---> Store in Vector DB\n",
    "\n",
    "The vector DB we're using here is Chroma DB. One the content is stored, the user question is then answered based on the ingested content\n",
    "\n",
    "#### User question ---> Retrieve relevant content from vector DB ---> LLM ---> Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501ffd66-06b6-4457-a612-9aba194c2858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292c79cc-7915-4374-bcf7-9a70a4f5830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "import bs4\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d484f2dc-38ed-4104-8d9f-d58e1e6c16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "system_prompt = (\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "                 \n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ffb2f5-1c0c-4259-8538-e628902631b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tool usage refers to the creation, modification, and utilization of external objects to perform tasks that go beyond an individual's physical and cognitive limits. It is a distinguishing characteristic of human beings and can significantly extend the capabilities of systems, such as equipping language models with external tools to enhance their functionality.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is tool usage?\"\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b9546-abc5-49b1-ab83-0c3f49dad6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
