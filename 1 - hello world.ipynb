{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3867e53f-be36-4181-9e2b-5539736f16fa",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "This notebook is designed to guide you through the process of building a Language Learning Model (LLM) application using Langchain and OpenAI's GPT-4 LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9bfe44-756c-4dea-8384-20b693c30715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37d4f2",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "Langchain is the most popular SDK for building LLM apps. It provides a great way to quickly perform actions you would need to build an app while giving you flexibility to switch between model providers easily without changing your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5ba49",
   "metadata": {},
   "source": [
    "### Large Language Model\n",
    "Large Language Models (LLMs) are AI systems trained to understand and generate human-like text based on the input they receive. They've been the engine behind the recent GenAI boom. We'll use OpenAI's LLM GPT-4o in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21561bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb5334",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "Building LLM apps can be a little tricky sometimes as they can feel like a black box. LangSmith is a great tool to help you understand what's going on inside the LLM. It's a simple command line tool that allows you to interact with the LLM in a more direct way. You can use it to train, evaluate, and generate text from your LLM. It's a great tool to help you understand how the apps is working and building annotation queues and datasets for future testing.\n",
    "\n",
    "We have enabled LangSmith by adding our LANGCHAIN_API_KEY in .env file and setting LANGCHAIN_TRACING_V2=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe1c12-60c5-42df-b57f-083bf9e509bb",
   "metadata": {},
   "source": [
    "### You first prompt!\n",
    "LLMs organize messages into different types\n",
    "SystemMessage is the prompt your provide to the LLM as it's role and function. This is where you would tell it what to do with the user input\n",
    "HumanMessage in the input that comes from the humans (read users) that the LLM then acts on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595e2e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿Cómo estás?', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 22, 'total_tokens': 29}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-15e63cc5-f506-44b4-81fb-137ed0cfdede-0', usage_metadata={'input_tokens': 22, 'output_tokens': 7, 'total_tokens': 29})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = \"Respond back to the user greeting in Spanish\"\n",
    "human_message = \"Hello AI!\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=human_message),\n",
    "]\n",
    "\n",
    "# Let's pass the system and human message to the LLM API and invoke it\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2cd481-6acb-4168-9c07-d8359f732347",
   "metadata": {},
   "source": [
    "### Output format\n",
    "A lot of times we just want the actual text output from the LLM and don't want all the metadata related to the call as users won't care about that. LangChain has an easy way to handle that with a concept called output parsers. This allows us to easily format output in different format based on our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c504e7-074c-42ab-abf3-699c5e459a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96254117-5e72-409c-be92-4e23b1cc9e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo estás?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = model | parser\n",
    "chain.invoke(messages)\n",
    "# This will only show the text response form the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98049b-b7eb-421f-930c-7e3226d301ea",
   "metadata": {},
   "source": [
    "### Switching models\n",
    "One of the big benefits of using an abstraction layer like LangChain is to easily swap out LLMs. We will now load and run the same messages on Anthropic Claude 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49057490-67bd-4b1a-aa16-38c190772983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "model = ChatAnthropic(model=\"claude-3-opus-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c1c2f7-4187-4401-a477-79be10c4deed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo estás?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = model | parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb25506-fbb2-4ca5-8dd6-2bf785dcd751",
   "metadata": {},
   "source": [
    "#### Review the traces\n",
    "You can now go view these traces in LangSmith. You will see 3:\n",
    "1. Initial call to OpenAI Chat model\n",
    "2. Second call to OpenAI chat model now with the additional string otput parser\n",
    "3. Call to Anthropic chat model with the string output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12be3a-cc86-467b-8e7e-c90a7d5604a4",
   "metadata": {},
   "source": [
    "#### Congratulations! You've completed Notebook 1 - Hello World\n",
    "Hope you've learned\n",
    "1. How to invoke a LLM API using LangChain\n",
    "2. How to use string output parser to only get the main output from the model\n",
    "3. Easily switch models and take advantage of abstration provided by LangChain\n",
    "\n",
    "Take a deep breath, stretch out a little and get ready for the second notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
